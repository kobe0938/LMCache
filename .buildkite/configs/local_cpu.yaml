workload:
  type: long-doc-qa
  num_docs:             8
  doc_len:              20000
  out_len:              100
  repeat_count:         2
  repeat_mode:          random
  shuffle_seed:         0
  max_inflight:         20
  sleep_after:          0
  expected_ttft_gain:   2
  expected_latency_gain: 2

docker:
  env:
    - "LMCACHE_CHUNK_SIZE=256"
    - "LMCACHE_LOCAL_CPU=True"
    - "LMCACHE_MAX_LOCAL_CPU_SIZE=5"

vllm:
  model: "meta-llama/Llama-3.2-1B-Instruct"
  args:
    - "--load-format"
    - "dummy"
